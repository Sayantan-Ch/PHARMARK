{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aeedca0",
   "metadata": {},
   "source": [
    "# A rudimentary build of the PHARMARK model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd246791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42f90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('brand_name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e012298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['Country'] == 'INDIA']['Drug name']\n",
    "df_train = df_train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc7faf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['\\n', \"'\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ','_','-','.','/','&','1','2','3','4','5','6','7','8','9','0','+','(',')','>','<','%','`']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2700fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, \"'\": 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27, ' ': 28, '_': 29, '-': 30, '.': 31, '/': 32, '&': 33, '1': 34, '2': 35, '3': 36, '4': 37, '5': 38, '6': 39, '7': 40, '8': 41, '9': 42, '0': 43, '+': 44, '(': 45, ')': 46, '>': 47, '<': 48, '%': 49, '`': 50}\n"
     ]
    }
   ],
   "source": [
    "# Set indices, to construct a word \n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9555da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_one_list = df_train.tolist()\n",
    "col_one_list = [i.lower() for i in col_one_list]\n",
    "col_one_list = [i.split(\" \") for i in col_one_list]\n",
    "col_one_list = [i[0] for i in col_one_list]\n",
    "col_one_list = set(col_one_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a79430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines: 4182\n"
     ]
    }
   ],
   "source": [
    "# Split into lines and get rid of empty lines \n",
    "lines = col_one_list\n",
    "lines = [line for line in lines if len(line)!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5247853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line with longest length: 33\n",
      "line with shorter length: 1\n"
     ]
    }
   ],
   "source": [
    "# Get length of longest word\n",
    "maxlen = len(max(lines, key=len)) + 15\n",
    "minlen = len(min(lines, key=len))\n",
    "\n",
    "print(\"line with longest length: \"+ str(maxlen))\n",
    "print(\"line with shorter length: \"+ str(minlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d5dd5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for line in lines:\n",
    "    # pre-padding with zeros\n",
    "    s = (maxlen - len(line))*'0' + line\n",
    "    sequences.append(s)\n",
    "    next_chars.append('\\n')\n",
    "    for it,j in enumerate(line):\n",
    "        if (it >= len(line)-1):\n",
    "            continue\n",
    "        s = (maxlen - len(line[:-1-it]))*'0' + line[:-1-it]\n",
    "        sequences.append(s)\n",
    "        next_chars.append(line[-1-it])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0414aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-2b5202f198e6>:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
      "<ipython-input-14-2b5202f198e6>:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "# Vectorization\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "for i, seq in enumerate(sequences):\n",
    "    for t, char in enumerate(seq):\n",
    "        if char != '0':\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad430261",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "max_names = 10\n",
    "\n",
    "def sample(preds):\n",
    "    \"\"\" function that sample an index from a probability array \"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = preds / np.sum(preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.random.choice(range(len(chars)), p = probas.ravel())\n",
    "\n",
    "def print_name_generated(name):\n",
    "    print(name, flush=True)\n",
    "def print_list_generated(lst):\n",
    "    print(lst, flush=True)\n",
    "    \n",
    "    \n",
    "def generate_new_names(*args):\n",
    "    print(\"----------Generating names----------\")\n",
    "\n",
    "    # Add pre-padding of zeros in the input.\n",
    "    sequence = ('{0:0>' + str(maxlen) + '}').format(prefix).lower()\n",
    "\n",
    "    # tmp variables\n",
    "    tmp_generated = prefix\n",
    "    list_outputs = list()\n",
    "\n",
    "    while (len(list_outputs) < max_names):\n",
    "\n",
    "        # Vectorize the input of the model.\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sequence):\n",
    "            if char != '0':\n",
    "                x_pred[0, t, char_indices[char]] = 1\n",
    "\n",
    "        # Predict the probabilities of the next char.\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "\n",
    "        # Chose one based on the distribution obtained in the output of the model.\n",
    "        next_index = sample(preds)\n",
    "        # Get the corresponding char.\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        # If the char is a new line character or the name start to be bigger than the longest word, \n",
    "        # try to add it to the list and reset temp variables.\n",
    "        if next_char == '\\n' or len(tmp_generated) > maxlen:\n",
    "            \n",
    "            # If the name generated is not in the list, append it and print it.\n",
    "            if tmp_generated not in list_outputs:\n",
    "                list_outputs.append(tmp_generated)\n",
    "                print_name_generated(tmp_generated)\n",
    "            # Reset tmp variables\n",
    "            sequence = ('{0:0>' + str(maxlen) + '}').format(prefix).lower()\n",
    "            tmp_generated = prefix\n",
    "        else:\n",
    "    \n",
    "            # Append the char to the sequence that we're generating.\n",
    "            tmp_generated += next_char\n",
    "            # Add pre-padding of zeros to the sequence generated and continue.\n",
    "            sequence = ('{0:0>' + str(maxlen) + '}').format(tmp_generated).lower()\n",
    "            \n",
    "    # Show the intersection of the words generated and your dataset. . \n",
    "    print(\"Set of words already in the dataset:\")\n",
    "    print_list_generated(set(lines).intersection(list_outputs))\n",
    "    \n",
    "    # Show the rate of how many repeated words you've created.\n",
    "    total_repited = len(set(lines).intersection(list_outputs))\n",
    "    total = len(list_outputs)\n",
    "    print(\"Rate of total invented words: \" + \"{:.2f}\".format((total-total_repited)/total))\n",
    "    print(\"-----------------End-----------------\")\n",
    "    \n",
    "# Function invoked at the end of each epoch. Prints generated names.\n",
    "callback = LambdaCallback(on_epoch_end=generate_new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9e2feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------Generating names----------\n",
      "hakove\n",
      "-pipyan\n",
      "d\n",
      "ivacina\n",
      "ugelas\n",
      "onetor\n",
      "ouma-p\n",
      "oflaceet-mx\n",
      "raldaaa\n",
      "slacpoki-smn\n",
      "Set of words already in the dataset:\n",
      "{'d'}\n",
      "Rate of total invented words: 0.90\n",
      "-----------------End-----------------\n",
      "251/251 - 15s - loss: 2.6439 - 15s/epoch - 58ms/step\n",
      "Epoch 2/10\n",
      "----------Generating names----------\n",
      "reno-rdrex\n",
      "rnuart\n",
      "khigele\n",
      "aciclas\n",
      "otexi\n",
      "ampitalupnus-as\n",
      "aleden\n",
      "acnijinimd\n",
      "lomradere-d\n",
      "limobes\n",
      "Set of words already in the dataset:\n",
      "set()\n",
      "Rate of total invented words: 1.00\n",
      "-----------------End-----------------\n",
      "251/251 - 12s - loss: 2.3110 - 12s/epoch - 49ms/step\n",
      "Epoch 3/10\n",
      "----------Generating names----------\n",
      "figlulda\n",
      "ioxorur\n",
      "icnoloee\n",
      "ordhirot\n",
      "restokin\n",
      "amonocic\n",
      "ib1enm-d\n",
      "iclora\n",
      "ravonin\n",
      "osoan\n",
      "Set of words already in the dataset:\n",
      "set()\n",
      "Rate of total invented words: 1.00\n",
      "-----------------End-----------------\n",
      "251/251 - 12s - loss: 2.2125 - 12s/epoch - 46ms/step\n",
      "Epoch 4/10\n",
      "----------Generating names----------\n",
      "asika5\n",
      "ycemp\n",
      "iburin\n",
      "uvarad\n",
      "restor\n",
      "anfestan\n",
      "iosiox\n",
      "ynime\n",
      "ulpivas\n",
      "fakopran\n",
      "Set of words already in the dataset:\n",
      "set()\n",
      "Rate of total invented words: 1.00\n",
      "-----------------End-----------------\n",
      "251/251 - 11s - loss: 2.1464 - 11s/epoch - 45ms/step\n",
      "Epoch 5/10\n",
      "----------Generating names----------\n",
      "ignara\n",
      "ecocal\n",
      "etoxin-ds-p\n",
      "artacri\n",
      "ulgrajoind-plu\n",
      "nem-fart\n",
      "knoast\n",
      "orthol\n",
      "oravil\n",
      "ingis-al\n",
      "Set of words already in the dataset:\n",
      "set()\n",
      "Rate of total invented words: 1.00\n",
      "-----------------End-----------------\n",
      "251/251 - 12s - loss: 2.0942 - 12s/epoch - 46ms/step\n",
      "Epoch 6/10\n",
      "----------Generating names----------\n",
      "umta\n",
      "edraytin\n",
      "celtoox\n",
      "openoz-d\n",
      "\n",
      "ontonib\n",
      "etor\n",
      "ustayan\n",
      "trimushidd\n",
      "elget\n",
      "Set of words already in the dataset:\n",
      "set()\n",
      "Rate of total invented words: 1.00\n",
      "-----------------End-----------------\n",
      "251/251 - 10s - loss: 2.0506 - 10s/epoch - 41ms/step\n",
      "Epoch 7/10\n",
      "----------Generating names----------\n",
      "rulide-sp\n",
      "temdac\n",
      "-cornes\n",
      "nimedo-lomt\n",
      "rekegrom\n",
      "xinini\n",
      "urinil\n",
      "rictren-th\n",
      "hanarsp\n",
      "ingraland\n",
      "Set of words already in the dataset:\n",
      "set()\n",
      "Rate of total invented words: 1.00\n",
      "-----------------End-----------------\n",
      "251/251 - 10s - loss: 2.0183 - 10s/epoch - 41ms/step\n",
      "Epoch 8/10\n",
      "----------Generating names----------\n",
      "mudotar\n",
      "ithesh\n",
      "xulorim\n",
      "etolant\n",
      "orajoces\n",
      "enpiparine\n",
      "ecanec-d-brus\n",
      "arserive\n",
      "yclove\n",
      "efren\n",
      "Set of words already in the dataset:\n",
      "set()\n",
      "Rate of total invented words: 1.00\n",
      "-----------------End-----------------\n",
      "251/251 - 10s - loss: 1.9835 - 10s/epoch - 41ms/step\n",
      "Epoch 9/10\n",
      "----------Generating names----------\n",
      "inox\n",
      "stolive\n",
      "insafem\n",
      "etoric-st\n",
      "ecumol\n",
      "ulifen-sq\n",
      "\n",
      "clodap\n",
      "manth\n",
      "eacotrim\n",
      "Set of words already in the dataset:\n",
      "set()\n",
      "Rate of total invented words: 1.00\n",
      "-----------------End-----------------\n",
      "251/251 - 10s - loss: 1.9633 - 10s/epoch - 40ms/step\n",
      "Epoch 10/10\n",
      "----------Generating names----------\n",
      "ramitiz\n",
      "comigesic\n",
      "ooflex\n",
      "arflapant\n",
      "loren-pc\n",
      "gonim\n",
      "efenic\n",
      "ibibad\n",
      "tratrogan\n",
      "lornum-ct\n",
      "Set of words already in the dataset:\n",
      "set()\n",
      "Rate of total invented words: 1.00\n",
      "-----------------End-----------------\n",
      "251/251 - 11s - loss: 1.9319 - 11s/epoch - 44ms/step\n"
     ]
    }
   ],
   "source": [
    "# model \n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "history = model.fit(x, y, batch_size=128, epochs=10, verbose=2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c02be304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                29696     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 51)                3315      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,011\n",
      "Trainable params: 33,011\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96eae6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Generating names----------\n",
      "antipas\n",
      "antifol\n",
      "antil-wase\n",
      "antetri\n",
      "antexine\n",
      "antrool\n",
      "antrag\n",
      "antaine\n",
      "antmor\n",
      "antimasi\n",
      "antopel\n",
      "anthrin\n",
      "antijod\n",
      "anthoking\n",
      "antidox\n",
      "antives\n",
      "antciflam-ssra\n",
      "antigesic\n",
      "antipm\n",
      "antrol\n",
      "Set of words already in the dataset:\n",
      "set()\n",
      "Rate of total invented words: 1.00\n",
      "-----------------End-----------------\n"
     ]
    }
   ],
   "source": [
    "prefix = \"ant\"\n",
    "max_names = 20\n",
    "\n",
    "generate_new_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae0796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a899369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eead5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e32eeef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d63a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fe340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4024a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b798cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d51569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf186b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d4a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f442ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
